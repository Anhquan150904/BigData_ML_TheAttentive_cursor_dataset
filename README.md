# üìò H∆∞·ªõng D·∫´n Thi·∫øt L·∫≠p M√¥i Tr∆∞·ªùng Ph√¢n T√≠ch D·ªØ Li·ªáu (Spark & Cassandra)

T√†i li·ªáu n√†y h∆∞·ªõng d·∫´n chi ti·∫øt c√°ch thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng ƒë·ªÉ ch·∫°y d·ª± √°n ph√¢n t√≠ch h√†nh vi ng∆∞·ªùi d√πng (Attentive Cursor Dataset) s·ª≠ d·ª•ng **PySpark**, **Cassandra (Docker)** v√† **Python 3.10** tr√™n Windows.

---

## üìã M·ª•c l·ª•c

1. [Y√™u c·∫ßu h·ªá th·ªëng & T·∫£i ph·∫ßn m·ªÅm](#1-y√™u-c·∫ßu-h·ªá-th·ªëng--t·∫£i-ph·∫ßn-m·ªÅm)
2. [C√†i ƒë·∫∑t & C·∫•u h√¨nh bi·∫øn m√¥i tr∆∞·ªùng](#2-c√†i-ƒë·∫∑t--c·∫•u-h√¨nh-bi·∫øn-m√¥i-tr∆∞·ªùng-quan-tr·ªçng)
3. [C√†i ƒë·∫∑t Database Cassandra b·∫±ng Docker](#3-c√†i-ƒë·∫∑t-database-cassandra-docker)
4. [Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng Python](#4-thi·∫øt-l·∫≠p-m√¥i-tr∆∞·ªùng-python)
5. [D·ªØ li·ªáu d·ª± √°n](#5-d·ªØ-li·ªáu-d·ª±-√°n)
6. [Ch·∫°y th·ª≠ Spark](#Ô∏è‚É£-ch·∫°y-th·ª≠-code)

---

## 1. Y√™u c·∫ßu h·ªá th·ªëng & T·∫£i ph·∫ßn m·ªÅm

Vui l√≤ng t·∫£i c√°c th√†nh ph·∫ßn sau (ch∆∞a c·∫ßn c√†i ƒë·∫∑t ngay):

### **A. Java Development Kit (JDK)**

* Phi√™n b·∫£n: **OpenJDK 11 (LTS)**
* Link: *https://adoptium.net/fr/temurin/releases?version=11&os=any&arch=any*
* Ch·ªçn file **.msi**, h·ªá ƒëi·ªÅu h√†nh **Windows x64**.

### **B. Apache Spark**

* Phi√™n b·∫£n: **Spark 3.5.1** (Pre-built for Hadoop 3.3)
* Link: *https://repo.huaweicloud.com/apache/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz*
* L∆∞u √Ω: File t·∫£i v·ªÅ ch·ªâ c·∫ßn **gi·∫£i n√©n**, kh√¥ng c·∫ßn c√†i ƒë·∫∑t.

### **C. Hadoop Winutils (Windows Only)**

* Phi√™n b·∫£n: **Hadoop 3.3.6**
* T·∫£i t·ª´ repo Winutils: *https://github.com/cdarlint/winutils/blob/master/hadoop-3.3.6/bin/winutils.exe*
* (N·∫øu c√≥) t·∫£i th√™m **hadoop.dll** (ƒë·∫∑t c√πng th∆∞ m·ª•c bin).

### **D. Python**

* Phi√™n b·∫£n: **Python 3.10.x**

---

## 2. C√†i ƒë·∫∑t & C·∫•u h√¨nh Bi·∫øn m√¥i tr∆∞·ªùng (Quan tr·ªçng)

N·∫øu l√†m sai b∆∞·ªõc n√†y, Spark s·∫Ω kh√¥ng ch·∫°y.

### **B∆∞·ªõc 2.1: C√†i ƒë·∫∑t Java**

* Ch·∫°y file c√†i JDK 11.
* Ghi l·∫°i ƒë∆∞·ªùng d·∫´n c√†i ƒë·∫∑t, v√≠ d·ª•:

  ```
  C:\Program Files\Eclipse Adoptium\jdk-11.0.x
  ```

### **B∆∞·ªõc 2.2: Gi·∫£i n√©n Spark**

* Gi·∫£i n√©n file Spark.
* ƒê·∫∑t v√†o th∆∞ m·ª•c g·ªçn g√†ng, v√≠ d·ª•:

  ```
  D:\Spark\spark-3.5.1-bin-hadoop3
  ```

### **B∆∞·ªõc 2.3: C√†i Winutils (Hadoop Home)**

1. T·∫°o th∆∞ m·ª•c:

   ```
   D:\Hadoop
   ```
2. T·∫°o ti·∫øp:

   ```
   D:\Hadoop\bin
   ```
3. Copy **winutils.exe** (v√† hadoop.dll n·∫øu c√≥) v√†o th∆∞ m·ª•c `bin`.

### **B∆∞·ªõc 2.4: C·∫•u h√¨nh Environment Variables**

M·ªü: **Edit the system environment variables** ‚Üí **Environment Variables**.

#### **T·∫°o System Variables m·ªõi**:

| Variable    | Value                                      |
| ----------- | ------------------------------------------ |
| JAVA_HOME   | C:\Program Files\Eclipse Adoptium\jdk-11.x |
| HADOOP_HOME | D:\Hadoop                                  |
| SPARK_HOME  | D:\Spark\spark-3.5.1-bin-hadoop3           |

#### **C·∫≠p nh·∫≠t PATH**:

Th√™m 3 d√≤ng sau:

```
%JAVA_HOME%\bin
%HADOOP_HOME%\bin
%SPARK_HOME%\bin
```

---

## 3. C√†i ƒë·∫∑t Database Cassandra (Docker)

### **K√©o image Cassandra**

```bash
docker pull cassandra:4.1
```

### **Ch·∫°y container**

```bash
docker run --name cass-node -d -p 9042:9042 cassandra:4.1
```

### **Ki·ªÉm tra container**

```bash
docker ps
```

N·∫øu th·∫•y tr·∫°ng th√°i **Up**, nghƒ©a l√† Cassandra ƒë√£ ch·∫°y.

### **(T√πy ch·ªçn) M·ªü CQLSH**

```bash
docker exec -it cass-node cqlsh
```

Tho√°t:

```
exit
```

---

## 4. Thi·∫øt l·∫≠p M√¥i tr∆∞·ªùng Python

### **B∆∞·ªõc 4.1: T·∫°o m√¥i tr∆∞·ªùng ·∫£o**

M·ªü PowerShell t·∫°i th∆∞ m·ª•c d·ª± √°n:

```powershell
py -3.10 --version
py -3.10 -m venv cassandra_env
```

### **B∆∞·ªõc 4.2: K√≠ch ho·∫°t m√¥i tr∆∞·ªùng ·∫£o**

```powershell
cassandra_env\Scripts\activate
```

### **B∆∞·ªõc 4.3: C√†i ƒë·∫∑t th∆∞ vi·ªán Python**

```powershell
pip install pyspark cassandra-driver pandas matplotlib seaborn numpy
```

---

## 5. D·ªØ li·ªáu d·ª± √°n

Dataset: **The Attentive Cursor Dataset** (GitLab).

---

## #Ô∏è‚É£ Ch·∫°y th·ª≠ code

T·∫°o file `src/analysis.py` v√† ch·∫°y th·ª≠ Spark:

```python
import os
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("TestSetup") \
    .master("local[*]") \
    .getOrCreate()

print("Spark Version:", spark.version)
print("Environment Setup Successful!")
```

---

üí° **N·∫øu th·∫•y in ra phi√™n b·∫£n Spark ‚Üí b·∫°n ƒë√£ setup th√†nh c√¥ng 100%!**
